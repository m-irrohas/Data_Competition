{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Requirement\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import sklearn.preprocessing as sp\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "def meanAbsoluteError(y_test, y_pred):\n",
    "    \"\"\"for val\"\"\"\n",
    "    loss = 0\n",
    "    for i in range(len(y_test)):\n",
    "        loss += abs(y_test[i] - y_pred[i])\n",
    "    loss /= len(y_test)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train・Testデータ抽出\n",
    "df_train= pd.read_csv(\"./dataset/pubg-train.csv\")\n",
    "df_test = pd.read_csv(\"./dataset/pubg-test.csv\")\n",
    "\n",
    "def get_more_train(df_train, df_test):\n",
    "\n",
    "    df_test[\"matchgroupId\"] = df_test[\"matchId\"]+df_test[\"groupId\"]\n",
    "    #[winPlacePerc, Id, matchId, groupId]\n",
    "    df_train = df_train[[\"winPlacePerc\",\"matchId\", \"groupId\"]].sort_values(by=[\"matchId\", \"groupId\"])\n",
    "    df_train[\"matchgroupId\"] = df_train[\"matchId\"] + df_train[\"groupId\"]\n",
    "    df_train = df_train.drop(columns=[\"matchId\", \"groupId\"])\n",
    "    df_train = df_train.groupby(\"matchgroupId\").mean()\n",
    "    \n",
    "    df_merge = pd.merge(df_test, df_train , on=\"matchgroupId\")\n",
    "    df_merge = df_merge.sort_values(by=[\"matchgroupId\"])\n",
    "    df_merge = df_merge[[\"Id\", \"winPlacePerc\"]]\n",
    "    df_merge2 = pd.merge(df_test, df_merge, on=\"Id\", how=\"left\")\n",
    "    df_merge2 = df_merge2.drop(columns=\"matchgroupId\")\n",
    "    df_merge2 = df_merge2.dropna(how=\"any\")\n",
    "    df_test = df_test.drop(columns=\"matchgroupId\")\n",
    "    return df_merge2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                  object\n",
       "groupId             object\n",
       "matchId             object\n",
       "assists              int64\n",
       "boosts               int64\n",
       "damageDealt        float64\n",
       "DBNOs                int64\n",
       "headshotKills        int64\n",
       "heals                int64\n",
       "killPlace            int64\n",
       "killPoints           int64\n",
       "kills                int64\n",
       "killStreaks          int64\n",
       "longestKill        float64\n",
       "matchDuration        int64\n",
       "matchType           object\n",
       "numGroups            int64\n",
       "rankPoints           int64\n",
       "revives              int64\n",
       "rideDistance       float64\n",
       "roadKills            int64\n",
       "swimDistance       float64\n",
       "teamKills            int64\n",
       "vehicleDestroys      int64\n",
       "walkDistance       float64\n",
       "weaponsAcquired      int64\n",
       "winPoints            int64\n",
       "winPlacePerc       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_more_train(df_train, df_test).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(356075, 28)\n",
      "(423172, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msy-o\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n",
      "C:\\Users\\msy-o\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\msy-o\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\msy-o\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:99: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "C:\\Users\\msy-o\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:100: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nY_train = df_train[\"winPlacePerc\"]\\nX_train = df_train.drop(columns=\"winPlacePerc\")\\nX_test = pd.DataFrame(df_test)\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train・Testデータ抽出\n",
    "#df_train= pd.read_csv(\"./dataset/pubg-train.csv\")\n",
    "#df_test = pd.read_csv(\"./dataset/pubg-test.csv\")\n",
    "\n",
    "def drop_and_add(df_train, df_test):\n",
    "    ep = 0.00001\n",
    "    col_to_drop_from_mp = [\"killPoints\", \"rankPoints\"]\n",
    "    df = pd.concat([df_train, df_test])\n",
    "    #df = df.drop(columns=[\"Id\"])\n",
    "    \n",
    "    df['_killPoints_rankpoints'] = df['rankPoints']+df['killPoints']\n",
    "    df[\"_totalDistance\"] = 0.25*df[\"rideDistance\"]+df[\"walkDistance\"]+df[\"swimDistance\"]\n",
    "    df['_walkDistance_heals_Ratio'] = df['walkDistance'] / df['heals']\n",
    "    df['_walkDistance_kills_Ratio'] = df['walkDistance'] / df['kills']\n",
    "    df['_kills_walkDistance_Ratio'] = df['kills'] / df['walkDistance']\n",
    "    df['_totalDistancePerDuration'] =  df[\"_totalDistance\"]/df[\"matchDuration\"]\n",
    "    df['_killPlace_kills_Ratio'] = df['killPlace']/df['kills']\n",
    "    df['_walkDistancePerDuration'] =  df[\"walkDistance\"]/df[\"matchDuration\"]\n",
    "    df['_walkDistancePerc'] = df.groupby('matchId')['walkDistance'].rank(pct=True).values\n",
    "    df['_killPerc'] = df.groupby('matchId')['kills'].rank(pct=True).values\n",
    "    df['_killPlacePerc'] = df.groupby('matchId')['killPlace'].rank(pct=True).values\n",
    "    df['_weaponsAcquiredPerc'] = df.groupby('matchId')['weaponsAcquired'].rank(pct=True).values\n",
    "    df['_walkDistance_kills_Ratio2'] = df['_walkDistancePerc'] / df['_killPerc']\n",
    "    df['_kill_kills_Ratio2'] = df['_killPerc']/df['_walkDistancePerc']\n",
    "    df['_killPlace_walkDistance_Ratio2'] = df['_walkDistancePerc']/df['_killPlacePerc']\n",
    "    df['_killPlace_kills_Ratio2'] = df['_killPlacePerc']/df['_killPerc']\n",
    "    df['_totalDistancePerc'] = df.groupby('matchId')['_totalDistance'].rank(pct=True).values\n",
    "    df['_walkDistance_kills_Ratio3'] = df['_walkDistancePerc'] / df['kills']\n",
    "    df['_walkDistance_kills_Ratio4'] = df['kills'] / df['_walkDistancePerc']\n",
    "    df['_walkDistance_kills_Ratio5'] = df['_killPerc'] / df['walkDistance']\n",
    "    df['_walkDistance_kills_Ratio6'] = df['walkDistance'] / df['_killPerc']\n",
    "    df.loc[df[\"rankPoints\"]==-1, \"rankPoints\"] =0\n",
    "    df[\"_headshot_ratio\"] = df[\"headshotKills\"]/df[\"kills\"]\n",
    "    df[\"_kills_over_headshotkills\"] = df[\"kills\"]/df[\"headshotKills\"]\n",
    "    df[\"_killstreak_ratio\"] = df[\"killStreaks\"]/df[\"kills\"]\n",
    "    \n",
    "    df[\"_weaponAcquired_per_distance\"] = df[\"_totalDistancePerc\"]/df[\"weaponsAcquired\"]\n",
    "    df[\"_weaponsAcquired_matchDuration\"] = df[\"weaponsAcquired\"]/df[\"matchDuration\"]\n",
    "    df[\"_kill_assists\"] = df[\"kills\"]*df[\"assists\"]\n",
    "    df[\"_healboosts\"] = df[\"heals\"]*df[\"boosts\"]\n",
    "    df[\"_skill\"] = df[\"headshotKills\"] * df[\"roadKills\"]\n",
    "    df[\"_killPlace\"] = -df[\"killPlace\"]\n",
    "    df[\"_hideDuration\"] = df[\"matchDuration\"]-(df[\"walkDistance\"]/6+df[\"rideDistance\"]/277+df[\"swimDistance\"])\n",
    "    \n",
    "    #df[\"_Zombie\"] = 0\n",
    "    #df[(df[\"kills\"]==0) & (df[\"_totalDistance\"]<50) & (df[\"weaponsAcquired\"]==0)][\"_Zombie\"] = -1\n",
    "    \n",
    "    #カテゴリラベルをone-hot化\n",
    "    le = sp.LabelEncoder()\n",
    "    le.fit(df[\"matchType\"].unique())\n",
    "    df[\"matchType\"] = le.fit_transform(df[\"matchType\"])\n",
    "    ohe = sp.OneHotEncoder()\n",
    "    enced = ohe.fit_transform(df[\"matchType\"].values.reshape(1, -1).transpose())\n",
    "    temp = pd.DataFrame(index=df[\"matchType\"].index, columns=\"matchType-\" + le.classes_, data=enced.toarray())\n",
    "    df = pd.concat([df, temp], axis=1)\n",
    "\n",
    "    df = df.drop(columns=[\"matchType\"])\n",
    "    #df = df.drop(columns=[\"matchId\", \"groupId\"])\n",
    "    \n",
    "    df_train = df[df[\"winPlacePerc\"].notnull()].reset_index(drop=True)\n",
    "    df_test = df[df[\"winPlacePerc\"].isnull()].drop([\"winPlacePerc\"], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    df_train[df_train == np.Inf] = np.NaN\n",
    "    df_test[df_test == np.Inf] = np.NaN\n",
    "    df_train[df_train == np.NINF] = np.NaN\n",
    "    df_test[df_test == np.NINF] = np.NaN\n",
    "    df_train.fillna(0, inplace=True)\n",
    "    df_test.fillna(0, inplace=True)\n",
    "    \n",
    "    \n",
    "    return df_train, df_test\n",
    "    \n",
    "def pre_process(df_train, df_test):\n",
    "    \"\"\"pandasから得たデータフレームの整形\n",
    "    Arg:\n",
    "        df_train(pd.DataFrame) 訓練用データフレーム\n",
    "        df_test(pd.DataFrame) テスト用データフレーム\n",
    "        \n",
    "    Return:\n",
    "        X_train(ndarray(float)) 訓練用の入力データ(加工済み)\n",
    "        Y_train(ndarray(float)) 訓練用のラベルデータ(確率:0~1)\n",
    "        X_test(ndarray(float)) テスト用の入力データ(X_trainと同じshape(-1))\n",
    "    \"\"\"\n",
    "    \n",
    "    df_train_more= get_more_train(df_train, df_test)\n",
    "    df_train = pd.concat([df_train, df_train_more])\n",
    "    print(df_train.shape)\n",
    "    df_test = df_test.drop(columns=\"matchgroupId\")\n",
    "    df_train ,df_test = drop_and_add(df_train, df_test)\n",
    "    df_train = df_train.drop(columns=[\"Id\",\"matchId\", \"groupId\"])\n",
    "    df_test = df_test.drop(columns=[\"Id\",\"matchId\", \"groupId\"])\n",
    "    Y_train = df_train[\"winPlacePerc\"]\n",
    "    X_train = df_train.drop(columns=[\"winPlacePerc\"])\n",
    "    X_test = df_test\n",
    "    \n",
    "    scaler_std = StandardScaler()\n",
    "    scaler_std.fit(X_train)\n",
    " \n",
    "    std_X_train = scaler_std.transform(X_train)\n",
    "    std_X_test = scaler_std.transform(X_test)\n",
    "    \n",
    "    return X_train, Y_train, X_test\n",
    "\n",
    "#df_train, df_test = drop_and_add(df_train, df_test)\n",
    "\n",
    "#df_train.to_csv(\"_df_train.csv\", index=False)\n",
    "#df_test.to_csv(\"_df_test.csv\", index=False)\n",
    "#_df_train = pd.read_csv(\"_df_train.csv\")\n",
    "#_df_test = pd.read_csv(\"_df_test.csv\")\n",
    "\n",
    "df_train= pd.read_csv(\"./dataset/pubg-train.csv\")\n",
    "print(df_train.shape)\n",
    "df_test = pd.read_csv(\"./dataset/pubg-test.csv\")\n",
    "X_train, Y_train, X_test = pre_process(df_train, df_test)\n",
    "\"\"\"\n",
    "Y_train = df_train[\"winPlacePerc\"]\n",
    "X_train = df_train.drop(columns=\"winPlacePerc\")\n",
    "X_test = pd.DataFrame(df_test)\n",
    "\"\"\"\n",
    "\n",
    "#X_train, Y_train, X_test = pre_process(df_train, df_test)\n",
    "# train test split\n",
    "\n",
    "#for validation\n",
    "#X_train, X_test, Y_train, Y_testval = train_test_split(X_train, Y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "#X_train = pd.DataFrame(X_train)\n",
    "#X_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFOLD = 3\n",
    "SEED = 0\n",
    "\n",
    "ntrain = X_train.shape[0]\n",
    "ntest = X_test.shape[0]\n",
    "\n",
    "kf = KFold(n_splits=NFOLD, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#各学習器のラッパー\n",
    "class SklearnWrapper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params[\"random_state\"] = seed\n",
    "        self.clf = clf(**params)\n",
    "        \n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x) #proba??\n",
    "    \n",
    "class LightGBMWrapper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params[\"feature_fraction_seed\"] = seed\n",
    "        params[\"bagging_seed\"] = seed\n",
    "        self.clf = clf(**params)\n",
    "        \n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "class XgbWrapper(object):\n",
    "    def __init__(self, seed=0, params=None):\n",
    "        self.params = params\n",
    "        self.params[\"seed\"] = seed\n",
    "        self.nrounds = params.pop(\"nrounds\", 250)\n",
    "        \n",
    "    def train(self, x_train, y_train):\n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        self.gbdt = xgb.train(self.params, dtrain, self.nrounds)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.gbdt.predict(xgb.DMatrix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLD, ntest))\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X_train)):\n",
    "        X_tr = X_train.iloc[train_index, :]\n",
    "        Y_tr = Y_train.iloc[train_index]\n",
    "        X_te = X_train.iloc[test_index, :]\n",
    "        \n",
    "        clf.train(X_tr, Y_tr)\n",
    "        \n",
    "        oof_train[test_index] = clf.predict(X_te)#regのinput\n",
    "        oof_test_skf[i, :] = clf.predict(X_test)\n",
    "        \n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_params = {\"n_jobs\":16,\n",
    "            \"n_estimators\":200,\n",
    "            \"max_features\":0.7,\n",
    "            \"max_depth\":12,\n",
    "            \"min_samples_leaf\":2,}\n",
    "\n",
    "rf_params = {\"n_jobs\":16,\n",
    "            \"n_estimators\":200,\n",
    "            \"max_features\":0.2,\n",
    "            \"max_depth\":12,\n",
    "            \"min_samples_leaf\":2,}\n",
    "\n",
    "xgb_params = {'seed': 0,\n",
    "              'colsample_bytree': 0.7,\n",
    "              'silent': 1,\n",
    "              'subsample': 0.7,\n",
    "              'learning_rate': 0.075,\n",
    "              'max_depth': 8,\n",
    "              'num_parallel_tree': 5,\n",
    "              'min_child_weight': 1,\n",
    "              'nrounds': 500}\n",
    "\n",
    "lightgbm_params1 = {'n_estimators':200,\n",
    "                   'learning_rate':0.05,\n",
    "                   'num_leaves':123,\n",
    "                   'colsample_bytree':0.8,\n",
    "                   'subsample':0.9,\n",
    "                   'max_depth':15,\n",
    "                   'reg_alpha':0.1,\n",
    "                   'reg_lambda':0.1,\n",
    "                   'min_split_gain':0.01,\n",
    "                   'min_child_weight':2,\n",
    "                   \"seed\":1}\n",
    "\n",
    "lightgbm_params2 ={\"n_estimators\":200,\n",
    "                  \"learning_rate\": 0.1,\n",
    "                  \"subsample\":0.8,\n",
    "                  \"subsample_freq\":4,\n",
    "                  \"colsample_bytree\":0.3,\n",
    "                  \"num_leaves\":150,\n",
    "                  \"seed\":1000,\n",
    "                  \"max_depth\":15,\n",
    "                  \"max_features\":\"sqrt\"}\n",
    "\n",
    "lightgbm_params3 = {\"n_estimators\": 500,\n",
    "                  \"learning_rate\":0.2,\n",
    "                  \"subsample\":0.8,\n",
    "                  \"subsample_freq\":6,\n",
    "                  \"colsample_bytree\":0.8,\n",
    "                  \"num_leaves\":50,\n",
    "                  \"seed\":1001,\n",
    "                  \"max_depth\":12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = XgbWrapper(seed=SEED, params=xgb_params)\n",
    "et = SklearnWrapper(clf=ExtraTreesRegressor, seed=SEED, params=et_params)\n",
    "rf = SklearnWrapper(clf=RandomForestRegressor, seed=SEED, params=rf_params)\n",
    "lg1 = LightGBMWrapper(clf=lgb.LGBMRegressor, seed=SEED, params=lightgbm_params1)\n",
    "lg2 = LightGBMWrapper(clf=lgb.LGBMRegressor, seed=SEED, params=lightgbm_params2)\n",
    "lg3 = LightGBMWrapper(clf=lgb.LGBMRegressor, seed=SEED, params=lightgbm_params3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LG1-MAE:0.06948295825294716\n",
      "LG2-MAE:0.0697829106222704\n",
      "LG3-MAE:0.07030123009885167\n"
     ]
    }
   ],
   "source": [
    "lg1_oof_train, lg1_oof_test = get_oof(lg1)\n",
    "print(\"LG1-MAE:{}\".format(mean_absolute_error(Y_train, lg1_oof_train)))\n",
    "\n",
    "\"\"\"\n",
    "et_oof_train, et_oof_test = get_oof(et)\n",
    "print(\"ET-MAE:{}\".format(mean_absolute_error(Y_train, et_oof_train)))\n",
    "xg_oof_train, xg_oof_test = get_oof(xg)\n",
    "print(\"XG-MAE:{}\".format(mean_absolute_error(Y_train, xg_oof_train)))\n",
    "rf_oof_train, rf_oof_test = get_oof(rf)\n",
    "print(\"RF-MAE:{}\".format(mean_absolute_error(Y_train, rf_oof_train)))\n",
    "\"\"\"\n",
    "\n",
    "lg2_oof_train, lg2_oof_test = get_oof(lg2)\n",
    "print(\"LG2-MAE:{}\".format(mean_absolute_error(Y_train, lg2_oof_train)))\n",
    "\n",
    "\n",
    "lg3_oof_train, lg3_oof_test = get_oof(lg3)\n",
    "print(\"LG3-MAE:{}\".format(mean_absolute_error(Y_train, lg3_oof_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-685eb800efd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlg1_oof_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlg2_oof_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlg3_oof_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlg1_oof_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlg2_oof_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlg3_oof_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#x_train = np.concatenate((lg1_oof_train, lg2_oof_train), axis=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#x_test = np.concatenate((lg1_oof_test, lg2_oof_test), axis=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "x_train = np.concatenate((lg1_oof_train, lg2_oof_train, lg3_oof_train), axis=1)\n",
    "x_test = np.concatenate((lg1_oof_test, lg2_oof_test, lg3_oof_test), axis=1)\n",
    "#x_train = np.concatenate((lg1_oof_train, lg2_oof_train), axis=1)\n",
    "#x_test = np.concatenate((lg1_oof_test, lg2_oof_test), axis=1)\n",
    "\n",
    "# train test split\n",
    "X_train, X_val, y_train, Y_val = train_test_split(x_train, Y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"{},{}\".format(x_train.shape, x_test.shape))\n",
    "\n",
    "lgb_train = lgb.Dataset(np.array(X_train),np.array(y_train))\n",
    "lgb_eval = lgb.Dataset(np.array(X_val),np.array(Y_val),reference=lgb_train)\n",
    "\n",
    "params={'learning_rate': 0.05,\n",
    "        \"n_estimators\" : 10000,\n",
    "        'objective':'poisson',\n",
    "        'metric':'mae',\n",
    "        'num_leaves': 31,\n",
    "        'verbose': 1,\n",
    "        'random_state':42,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'feature_fraction': 0.7\n",
    "       }\n",
    "\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=7500,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=30,\n",
    "               )\n",
    "\n",
    "def meanAbsoluteError(y_test, y_pred):\n",
    "    loss = 0\n",
    "    for i in range(len(y_test)):\n",
    "        loss += abs(y_test[i] - y_pred[i])\n",
    "    loss /= len(y_test)\n",
    "    return loss\n",
    "\n",
    "Y_pred= gbm.predict(np.array(x_test), num_iteration=gbm.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(meanAbsoluteError(Y_testval.values, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def after_process(Y_pred):\n",
    "    \"\"\"予測結果を提出用に整形し，csvで保存\n",
    "    Arg:\n",
    "        Y_pred_submit(ndarray(float))\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df_submit = pd.DataFrame(Y_pred, columns=[\"winPlacePerc\"])\n",
    "    df_submit.loc[df_submit['winPlacePerc'] < 0, 'winPlacePerc'] = 0.0\n",
    "    df_submit.loc[df_submit['winPlacePerc'] > 1.0, 'winPlacePerc'] = 1.0\n",
    "    df_submit.to_csv(\"3predict_test.csv\", index=False)\n",
    "    print(\"保存done\")\n",
    "    return None\n",
    "\n",
    "#after_process(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_match_group_data(Y_pred, df_test, df_train):\n",
    "    \"\"\"matchとgroupが一致するデータを統合\"\"\"\n",
    "    df_pred = pd.DataFrame(Y_pred, columns=[\"winPlacePerc\"])\n",
    "    df_pred = pd.concat([df_pred, df_test[[\"Id\", \"matchId\", \"groupId\",\n",
    "                                           \"kills\", \"_totalDistance\", \"weaponsAcquired\"]]], axis=1)\n",
    "    df_pred[\"matchgroupId\"] = df_pred[\"matchId\"]+df_pred[\"groupId\"]\n",
    "    df_pred = df_pred.drop(columns=[\"matchId\", \"groupId\"])\n",
    "    #Remove Zombi as 0\n",
    "    df_pred[(df_pred[\"kills\"]==0) & (df_pred[\"_totalDistance\"]<30) & (df_pred[\"weaponsAcquired\"]==0)][\"winPlacePerc\"]=0.025\n",
    "    \n",
    "    \n",
    "    #[winPlacePerc, Id, matchId, groupId]\n",
    "    df_train = df_train[[\"winPlacePerc\",\"matchId\", \"groupId\"]].sort_values(by=[\"matchId\", \"groupId\"])\n",
    "    df_train[\"matchgroupId\"] = df_train[\"matchId\"] + df_train[\"groupId\"]\n",
    "    df_train = df_train.drop(columns=[\"matchId\", \"groupId\"])\n",
    "    df_train = df_train.rename(columns={\"winPlacePerc\":\"wpp_acc\"})\n",
    "    df_train = df_train.groupby(\"matchgroupId\").mean()\n",
    "    \n",
    "    df_merge = pd.merge(df_pred, df_train , on=\"matchgroupId\")\n",
    "    df_merge = df_merge.sort_values(by=[\"matchgroupId\"])\n",
    "    df_merge = df_merge[[\"Id\", \"wpp_acc\"]]\n",
    "    df_merge2 = pd.merge(df_pred, df_merge, on=\"Id\", how=\"left\")\n",
    "    \n",
    "    df_submit = df_merge2[\"wpp_acc\"]\n",
    "    df_pred = df_pred[\"winPlacePerc\"]\n",
    "    print(df_pred.shape)\n",
    "    \n",
    "    df_submit = df_submit.values\n",
    "    print(df_submit.shape)\n",
    "    df_pred = df_pred.values\n",
    "    \n",
    "    \n",
    "    for i in range(len(df_submit)):\n",
    "        if np.isnan(df_submit[i]):\n",
    "            df_submit[i] = df_pred[i]\n",
    "\n",
    "    \n",
    "    #delete Zombie\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(df_submit)\n",
    "    \n",
    "    \"\"\"\n",
    "    matchgroupId2wpp = {}\n",
    "    for i in range(df_merge.shape[0]):\n",
    "        matchgroundId = df_merge[\"matchgroupId\"][i]\n",
    "        wpp_acc = df_merge[\"wpp_acc\"][i]\n",
    "        matchgroupId2wpp[matchgroundId] = wpp_acc\n",
    "    \"\"\"\n",
    "    \n",
    "    return df_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msy-o\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89019,)\n",
      "(89019,)\n",
      "保存done\n"
     ]
    }
   ],
   "source": [
    "df_train= pd.read_csv(\"./dataset/pubg-train.csv\")\n",
    "df_test = pd.read_csv(\"./dataset/pubg-test.csv\")\n",
    "df_test[\"_totalDistance\"] = 0.25*df_test[\"rideDistance\"]+df_test[\"walkDistance\"]+df_test[\"swimDistance\"]\n",
    "Y_submit = add_match_group_data(Y_pred, df_test, df_train)\n",
    "after_process(Y_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00469146]\n"
     ]
    }
   ],
   "source": [
    "df_sota = pd.read_csv(\"now_sota0487.csv\")\n",
    "Y_sota = df_sota.values\n",
    "print(meanAbsoluteError(Y_sota, Y_submit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
